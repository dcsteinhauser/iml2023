{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch import nn\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a linear model using the torch nn module.\n",
    "class RidgeRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(RidgeRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# wrapper for our data.\n",
    "class DataSet(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        data = pd.read_csv(\"train.csv\")\n",
    "        self.X = torch.as_tensor(data.iloc[:,1:].values).float()\n",
    "        self.y = torch.as_tensor(data.iloc[:,0].values).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "def pred_loss_fn(outputs, labels, alpha, weights):\n",
    "    return torch.sum(torch.square(outputs-labels)) + alpha*torch.sum(torch.square(weights))\n",
    "\n",
    "def train(model, data_loader, optimizer, num_epochs, alpha):\n",
    "    for epoch in range(num_epochs):\n",
    "        for X,y in data_loader:\n",
    "            model_outputs = model(X)\n",
    "            model_loss = pred_loss_fn(model_outputs, y, alpha, model.linear.weight)\n",
    "            optimizer.zero_grad()\n",
    "            model_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        X, y = next(iter(data_loader))\n",
    "\n",
    "def test(model, data_loader, gen_loss_fn):\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X,y in data_loader:\n",
    "            output = model(X).flatten()\n",
    "            loss+=gen_loss_fn(output,y).item()\n",
    "    return loss\n",
    "    \n",
    "def kfcvalidation(model, data, batch_size=5, alpha=0.1, learning_rate=0.0000000001, num_epochs=100):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "    kfold = KFold(n_splits=10, shuffle=True)\n",
    "    gen_loss_fn = nn.MSELoss()\n",
    "    overall_loss = 0\n",
    "\n",
    "    for fold, (train_indices, test_indices) in enumerate(kfold.split(data)):\n",
    "        train_samples = SubsetRandomSampler(train_indices)\n",
    "        test_samples = SubsetRandomSampler(test_indices)\n",
    "\n",
    "        # the DataLoader allows us to iterate over batch_size pairs of X and y.\n",
    "        train_data_loader = DataLoader(data, batch_size=batch_size,sampler=train_samples)\n",
    "        test_data_loader = DataLoader(data, batch_size=len(test_indices),sampler=SubsetRandomSampler)\n",
    "\n",
    "        print(f'Fold: {fold}, commencing training')\n",
    "        print(f'-----------------------------------')\n",
    "\n",
    "        train(model, train_data_loader, optimizer, num_epochs, alpha)\n",
    "        \n",
    "        fold_loss = test(model, train_data_loader, gen_loss_fn)\n",
    "        print(f'Loss for this fold is: {fold_loss}')\n",
    "        print(f'-----------------------------------')\n",
    "        overall_loss+=fold_loss\n",
    "    \n",
    "    return overall_loss/10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0, commencing training\n",
      "-----------------------------------\n",
      "Loss for this fold is: 3003.4609565734863\n",
      "-----------------------------------\n",
      "Fold: 1, commencing training\n",
      "-----------------------------------\n",
      "Loss for this fold is: 2466.7358798980713\n",
      "-----------------------------------\n",
      "Fold: 2, commencing training\n",
      "-----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[187], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[39m=\u001b[39m DataSet(\u001b[39m\"\u001b[39m\u001b[39mtrain.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m RidgeRegressionModel(\u001b[39m13\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m kfcvalidation(model\u001b[39m=\u001b[39;49mmodel, data\u001b[39m=\u001b[39;49mdata)\n",
      "Cell \u001b[0;32mIn[185], line 60\u001b[0m, in \u001b[0;36mkfcvalidation\u001b[0;34m(model, data, batch_size, alpha, learning_rate, num_epochs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFold: \u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m, commencing training\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m-----------------------------------\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m train(model, train_data_loader, optimizer, num_epochs, alpha)\n\u001b[1;32m     62\u001b[0m fold_loss \u001b[39m=\u001b[39m test(model, train_data_loader, gen_loss_fn)\n\u001b[1;32m     63\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLoss for this fold is: \u001b[39m\u001b[39m{\u001b[39;00mfold_loss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[185], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, optimizer, num_epochs, alpha)\u001b[0m\n\u001b[1;32m     29\u001b[0m model_outputs \u001b[39m=\u001b[39m model(X)\n\u001b[1;32m     30\u001b[0m model_loss \u001b[39m=\u001b[39m pred_loss_fn(model_outputs, y, alpha, model\u001b[39m.\u001b[39mlinear\u001b[39m.\u001b[39mweight)\n\u001b[0;32m---> 31\u001b[0m optimizer\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[1;32m     32\u001b[0m model_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     33\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/iml2023/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:456\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[39mif\u001b[39;00m foreach:\n\u001b[1;32m    455\u001b[0m     per_device_and_dtype_grads \u001b[39m=\u001b[39m defaultdict(\u001b[39mlambda\u001b[39;00m: defaultdict(\u001b[39mlist\u001b[39m))\n\u001b[0;32m--> 456\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_zero_grad_profile_name):\n\u001b[1;32m    457\u001b[0m     \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    458\u001b[0m         \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m group[\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[0;32m~/Documents/iml2023/.venv/lib/python3.10/site-packages/torch/autograd/profiler.py:507\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting():\n\u001b[1;32m    506\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m--> 507\u001b[0m         torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49m_record_function_exit\u001b[39m.\u001b[39;49m_RecordFunction(record)\n\u001b[1;32m    508\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    509\u001b[0m     torch\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[0;32m~/Documents/iml2023/.venv/lib/python3.10/site-packages/torch/_ops.py:287\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 287\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = DataSet(\"train.csv\")\n",
    "model = RidgeRegressionModel(13,1)\n",
    "\n",
    "kfcvalidation(model=model, data=data)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
